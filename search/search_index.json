{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Snakemake workflow: <code>pure-faf</code>","text":"<p>A Snakemake workflow for <code>VCF purification through formalin artefact filtering</code></p>"},{"location":"#authors","title":"Authors","text":"<ul> <li> <p>Fatma Rabia Fidan Ba\u015f</p> <ul> <li>The Francis Crick Institute</li> <li>ORCID profile</li> </ul> </li> <li> <p>Brian Hanley</p> <ul> <li>The Francis Crick Institute</li> <li>ORCID profile</li> </ul> </li> <li> <p>Husayn Pallikonda</p> <ul> <li>The Francis Crick Institute</li> <li>ORCID profile</li> </ul> </li> <li> <p>Irene Lobon </p> <ul> <li>AstraZeneca</li> <li>ORCID profile</li> </ul> </li> </ul>"},{"location":"#references","title":"References","text":"<p>K\u00f6ster, J., M\u00f6lder, F., Jablonski, K. P., Letcher, B., Hall, M. B., Tomkins-Tinch, C. H., Sochat, V., Forster, J., Lee, S., Twardziok, S. O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., &amp; Nahnsen, S. Sustainable data analysis with Snakemake. F1000Research, 10:33, 10, 33, 2021. https://doi.org/10.12688/f1000research.29032.2.</p>"},{"location":"input/","title":"Input data","text":""},{"location":"input/#you-need-these-following-files","title":"You need these following files:","text":"<ol> <li>Mutect2 VCF files (with AD, DP and ROQ annotations)</li> <li>Strelka2 indel VCF (or any other secondary variant caller should theoratically work)</li> <li>Sample bam/cram file</li> <li>Human reference genome fasta you used for alignment and variant calling</li> </ol>"},{"location":"input/#you-need-the-following-information","title":"You need the following information:","text":"<ol> <li>tumour and normal sample names</li> <li>Sequencing read length</li> <li>Sequencing adapters</li> </ol> <p>Using the input files and information, you need to create a sample sheet as follows:</p> tumour_name normal_name Mutect2_vcf Strelka2_indel_vcf tumour_bam_cram sequencing_read_length sequencing_adapter_1 sequencing_adapter_2 K1058_K1058_HB005_D02 K1058_K1058_BC001_D01 path/to/ROQ_flagged_K1058_HB005_D02.vcf.gz path/to/K1058_HB005_D02_vs_K1058_BC001_D01/K1058_HB005_D02_vs_K1058_BC001_D01.strelka.somatic_indels.vcf.gz path/to/K1058_HB005_D02/K1058_HB005_D02.recal.cram 300 AGCGAGAT-CCAGTATC AGCGAGAT-CCAGTATC K1058_K1058_HB003_D02 K1058_K1058_BC001_D01 path/to/ROQ_flagged_K1058_HB003_D02.vcf.gz path/to/K1058_HB003_D02_vs_K1058_BC001_D01/K1058_HB003_D02_vs_K1058_BC001_D01.strelka.somatic_indels.vcf.gz path/to/K1058_HB003_D02/K1058_HB003_D02.recal.cram 300 CTTGCTAG-TCATCTCC CTTGCTAG-TCATCTCC"},{"location":"input/#parameters","title":"Parameters","text":"<p>You then need to configure the workflow by updating the config/config.yaml. This table lists all parameters that can be used to run the workflow. All parameters except exclude_samples are mandatory.</p> parameter type details VAULT setup samplesheet path str path to samplesheet. ref_genome ref_genome_version str choose one of GRCh38 or GRCh37, case-insensitive. ref_genome_fasta str path to the human reference genome. VCF quality filter thresholds AD num minimum alternative allele count 4 ROQ num ROQ threshold for read orientation quality 20 DP num minimum read depth 0 - no seperate DP filtering PON parameters PON_alpha num Beta test significance level 0.05 Microsec parameters AD_tier_threshold num maximum AD for low-confidence tier 10 pval_threshold_high_AD num pvalue threshold for <code>fun_analysis</code> function for high-confidence tier 1e-4 pval_threshold_low_AD num pvalue threshold for <code>fun_analysis</code> function for low-confidence tier 1e-6 Post-filter thresholds VAF array a list of VAF values you want to apply after formalin filtering. All will appear in the QC plot. Other parameters odir str output directory. This will appear under results/ exclude_samples array a list of sample names you want to exclude from the analysis. Default is empty list."},{"location":"output/","title":"Output files and structure","text":"<p>The main output of the workflow are the formalin filtered VCF files located in <code>results/odir/filtered_vcf/</code>.</p> <pre><code>results/odir/\n    \u251c\u2500\u2500 logs\n        \u251c\u2500\u2500 1-step1_sample1.log\n        \u251c\u2500\u2500 2-step2_sample1.log\n        \u251c\u2500\u2500 ...\n    \u251c\u2500\u2500 filtered_vcf #MAIN OUTPUT FOLDER\n        \u251c\u2500\u2500 1-sample1_msec_all_filtered.vcf.gz #passing all microsec filters\n        \u251c\u2500\u2500 1-sample2_vault_filtered.vcf.gz #filtering creteria used in VAULT paper\n        \u251c\u2500\u2500 1-sample1__vault_plus_SR_filtered.vcf.gz #VAULT createria + simple repeat filter (relevant if whole genome data, otherwise almost no difference)\n        \u251c\u2500\u2500 2-..... #various VAF filtered VCFs\n        \u251c\u2500\u2500 2-..... #various VAF filtered VCFs\n    \u251c\u2500\u2500 microsec_input # sample info and mutation info files for microsec\n    \u251c\u2500\u2500 microsec_output #microsec tsv output files for samples and tiers\n    \u251c\u2500\u2500 ffpe_sig #QC plots and tables using FFPEsig repaired and unrepaired signatures\n    \u251c\u2500\u2500 temp # intermediate files. Most is automatically deleted after workflow completion. Remaining ones may be useful or can be deleted manually.\n    \u2514\u2500\u2500 PON # custom RepSeq panel of normals annotation and beta distribution testing results. Kind of intermediate, can be deleted if not needed.\n</code></pre>"},{"location":"pure-faf-method/","title":"pure-faf method","text":""},{"location":"pure-faf-method/#workflow-overview","title":"Workflow overview","text":"<p>This workflow is a best-practice workflow for variant filtering of formalin fixed tumour samples. The workflow is built using snakemake and consists of the following steps:</p> <ol> <li>Annotating rhe VCF with a custom panel of normals (PON) This PON has been created from RepSeq samples. Beta distribution testing is performed to identify variants that likely to come from PON.</li> <li>Applying a general quality filter with creteria including PON, AD and ROQ</li> <li>Splitting variants into confidence tiers using an AD threshold</li> <li>Intersecting low-confidence indels with a second tool's indel calls. Intersection is indel type-specific (insertion or deletion), and only position-based (alt allele doesn't matter).</li> <li>Running microsec twice with different parameters for low-confidence and high-confidence tiers</li> <li>Merging the results back annotating VCF with microsec results</li> <li>Applyying different filtering methods on the annotated VCF</li> <li>Applying a post-filter VAF filter </li> <li>Quality control using FFPEsig repaired and unrepaired signatures</li> </ol>"},{"location":"usage/crick_usage/","title":"Usage from within the Francis Crick Institute","text":"<p>NOTE: you need to run the main snakemake job on the login node. You need to use tmux or screen sessions for this purpose. In order to reattach a session, you need to be on the same login node where you created the session (login006 or login007).</p> <p>You can use the environment module system to load singularity (and conda if not already loaded):</p>"},{"location":"usage/crick_usage/#1-load-anaconda","title":"1. load anaconda:","text":"<pre><code>ml load Anaconda3/2024.10-1\nconda init\n</code></pre>"},{"location":"usage/crick_usage/#2-you-can-then-create-and-activate-a-conda-environment-for-snakemake","title":"2. you can then create and activate a conda environment for snakemake:","text":"<pre><code>conda config --set channel_priority strict\nconda create --name pure-faf -c conda-forge -c bioconda snakemake cyvcf2 snakemake-executor-plugin-slurm\n\n# or you can pip install snakemake-executor-plugin-slurm after creating the conda environment:\n# pip install snakemake-executor-plugin-slurm\n</code></pre>"},{"location":"usage/crick_usage/#3-create-a-tmux-or-screen-session-and-activate-the-conda-environment","title":"3. create a tmux or screen session and activate the conda environment:","text":"<pre><code>screen -S pure-faf_session\nconda activate pure-faf\n</code></pre>"},{"location":"usage/crick_usage/#4-load-singularity","title":"4. load singularity:","text":"<pre><code>ml load Singularity/3.11.3\n</code></pre> <p>Then follow the instructions in the general usage guide to clone the repository, configure and run the workflow. To see an example of snakemake profile configuration for slurm, see turajlic lab usage.</p>"},{"location":"usage/general_usage/","title":"Usage","text":"<p>The usage of this workflow is described in the Snakemake Workflow Catalog.</p> <p>If you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this repository or its DOI.</p>"},{"location":"usage/general_usage/#0-dependencies","title":"0) Dependencies","text":"<p>You need <code>snakemake</code>, <code>cyvcf2</code>, <code>conda</code> and <code>singularity</code> to run this workflow.</p> <pre><code>conda create --name pure-faf -c conda-forge -c bioconda snakemake singularity cyvcf2\nconda activate pure-faf\n</code></pre> <p>If you are running this workflow on an HPC environment, make sure to also install the correct executor plugin. For example:</p> <pre><code>pip install snakemake-executor-plugin-slurm\n</code></pre>"},{"location":"usage/general_usage/#1-clone-the-repository","title":"1) clone the repository","text":"<pre><code>git clone https://github.com/rabiafidan/pure-faf.git\n</code></pre> <p>To run the workflow from command line, change the working directory.</p> <pre><code>cd path/to/pure-faf\n</code></pre>"},{"location":"usage/general_usage/#2-configure-the-pipeline","title":"2) configure the pipeline","text":"<p>Adjust options in the default config file <code>config/config.yaml</code>.</p>"},{"location":"usage/general_usage/#3-run-the-pipeline","title":"3) run the pipeline","text":"<p>Before running the complete workflow, you can perform a dry run using:</p> <pre><code>snakemake --dry-run --use-conda --use-singularity\n</code></pre> <p>To run the workflow locally with test files using conda and singularity:</p> <pre><code>snakemake --cores 2 --use-conda --use-singularity\n</code></pre> <p>you can add <code>--no-temp</code> flag if you need to keep every intermidate file (for debugging purposes).</p> <p>You can also run the workflow on an HPC environment using an executor plugin. See snakemake documentation for more details on how to set up executor plugins. See turajlic lab usage for an example of slurm profile configuration.</p>"},{"location":"usage/turajlic_lab_usage/","title":"Usage from within the Turajlic Lab","text":"<p>NOTE: you need to run the main snakemake job on the login node. You need to use tmux or screen sessions for this purpose. In order to reattach a session, you need to be on the same login node where you created the session (login006 or login007).</p> <p>You can use the environment module system to load singularity (and conda if not already loaded):</p> <ol> <li>load anaconda:</li> </ol> <pre><code>ml load Anaconda3/2024.10-1\nconda init\n</code></pre> <ol> <li>you can then create and activate a conda environment for snakemake:</li> </ol> <pre><code>conda config --set channel_priority strict\nconda create --name pure-faf -c conda-forge -c bioconda snakemake cyvcf2 snakemake-executor-plugin-slurm\n\n# or you can pip install snakemake-executor-plugin-slurm after creating the conda environment:\n# pip install snakemake-executor-plugin-slurm\n</code></pre> <ol> <li>clone the repository</li> </ol> <pre><code>git clone https://github.com/rabiafidan/pure-faf.git\n</code></pre> <p>To run the workflow from command line, change the working directory.</p> <pre><code>cd path/to/pure-faf\n</code></pre> <ol> <li> <p>configure the pipeline Adjust options in the default config file <code>config/config.yaml</code>.</p> </li> <li> <p>create a tmux or screen session and activate the conda environment:</p> </li> </ol> <pre><code>screen -S pure-faf_session\nconda activate pure-faf\n</code></pre> <ol> <li>load singularity:</li> </ol> <pre><code>ml load Singularity/3.11.3\n</code></pre> <ol> <li>run the pipeline</li> </ol> <p>Before running the complete workflow, you can perform a dry run using:</p> <pre><code>snakemake --dry-run --executor slurm --profile /nemo/project/proj-tracerX/working/PIPELINES/Snakemake -p\n</code></pre> <p>To run the workflow</p> <pre><code>snakemake --executor slurm --profile /nemo/project/proj-tracerX/working/PIPELINES/Snakemake -p\n</code></pre> <p>you can add <code>--no-temp</code> flag if you need to keep every intermidate file (for debugging purposes).</p>"},{"location":"usage/turajlic_lab_usage/#profile-configuration","title":"profile configuration","text":"<p>Our Turajlic lab profile configuration is as pasted below for reference. For other labs, you need to change your slurm_account name and you need to change the singularity-args to make sure it binds all the necessary paths for your lab.</p> <pre><code>cat /nemo/project/proj-tracerX/working/PIPELINES/Snakemake/config.yaml \n</code></pre> <pre><code>executor: slurm\nlatency-wait: 80\nrerun-triggers: mtime\nkeep-going: true\nrerun-incomplete: true\nsingularity-args: '--bind /tmp,/flask/reference/Genomics/aws-igenomes/Homo_sapiens/,/camp/project/proj-tracerX/,/nemo/project/proj-tracerX/,/nemo/ess-manifest/proj-turajlics-rcc,/nemo/project/proj-turajlics-wgs --env GITHUB_PAT=${GITHUB_PAT}'\njobs: 70\nslurm-keep-successful-logs: true\nslurm-delete-logfiles-older-than: 0\nslurm-requeue: true\nslurm-efficiency-report: true\nslurm-efficiency-report-path: results/.efficiency_reports/\nuse-conda: true\nuse-singularity: true  \nuse-envmodules: true\n\ndefault-resources:\n    slurm_partition: \"ncpu\"\n    slurm_account: \"u_turajlics\"\n    runtime: 5000\n    mem_mb_per_cpu: 4000\n    slurm_cpus_per_task: 1 \n</code></pre>"}]}